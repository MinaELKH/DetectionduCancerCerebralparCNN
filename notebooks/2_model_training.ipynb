{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "244bafaf",
   "metadata": {},
   "source": [
    "## Conception du Mod√®le CNN\n",
    "\n",
    "1_ D√©finir l‚Äôarchitecture CNN (Conv2D + MaxPooling + Dropout + Dense) avec des param√®tres optimaux.\n",
    "\n",
    "2_ Choisir les meilleures fonctions d‚Äôactivation pour les couches cach√©es et la couche de sortie.\n",
    "\n",
    "**** pour les couches cach√©es :Relu\n",
    "     formule : f(x)= max(0,x)\n",
    "     on a choisi relu car \n",
    "     _ on active juste les neurones pertinents on prend les valeur positives et pour les valeurs negatives devient 0\n",
    "     _ rapide a calculer : accelere l'apprentisssage\n",
    "     _ evite les gradients tres petits \n",
    "**** pour la couche de sortie : Softmax\n",
    "     _ transforme les sorties en probablit√©s pour chaque classe\n",
    "     _ la somme de toutes les probabilites =1\n",
    "     _ideal pour classifiication multi-classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d6e144",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "import numpy as np\n",
    "\n",
    "num_classes = len(np.unique(labels))  # √† d√©finir selon ton dataset\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d71491f",
   "metadata": {},
   "source": [
    "3_ Compiler le mod√®le (ex. : Optimiseur Adam, fonction de perte categorical_crossentropy).\n",
    "\n",
    "=> Analyse :  \n",
    "\n",
    "pour optimiser notre modele on fait recours a optimiseur adm ainsi que la fonction de perte \n",
    "on commence par Categorical Crossentropy qui mesure l‚Äôerreur entre la pr√©diction du mod√®le et le vrai label.\n",
    "puis optimiseur  Adam utilise cette erreur pour ajuster les poids du r√©seau et am√©liorer la pr√©diction.\n",
    "\n",
    "**** Adam (optimiseur)\n",
    "\n",
    "Quand il intervient ?\n",
    "\n",
    "Pendant l‚Äôentra√Ænement du mod√®le, apr√®s chaque batch d‚Äôimages.\n",
    "\n",
    "Son r√¥le : mettre √† jour les poids des neurones pour que le mod√®le fasse moins d‚Äôerreurs sur tes images IRM.\n",
    "\n",
    "Exemple :\n",
    "\n",
    "Tu passes une image IRM d‚Äôun gliome dans ton CNN.\n",
    "\n",
    "Le mod√®le pr√©dit : [0.2, 0.1, 0.6, 0.1] (probabilit√©s pour [glioma, meningioma, notumor, pituitary])\n",
    "\n",
    "Le vrai label (one-hot) est [1, 0, 0, 0]\n",
    "\n",
    "Adam regarde la diff√©rence entre la pr√©diction et la v√©rit√© ‚Üí et ajuste tous les filtres/convolutions dans le r√©seau pour que la prochaine fois, le mod√®le pr√©dit mieux.\n",
    "\n",
    "\n",
    "\n",
    "**** Categorical Crossentropy (fonction de perte)\n",
    "\n",
    "Quand elle intervient ?\n",
    "\n",
    "Toujours pendant l‚Äôentra√Ænement, juste avant qu‚ÄôAdam mette √† jour les poids.\n",
    "\n",
    "Elle calcule combien la pr√©diction du mod√®le est mauvaise par rapport au vrai label.\n",
    "\n",
    "Exemple avec ton brief :\n",
    "\n",
    "M√™me image IRM : vrai label [1, 0, 0, 0]\n",
    "\n",
    "Mod√®le pr√©dit [0.2, 0.1, 0.6, 0.1]\n",
    "\n",
    "Crossentropy calcule ‚Äúperte = 1.609‚Äù (exemple num√©rique) ‚Üí plus la pr√©diction est loin du vrai label, plus la perte est grande\n",
    "\n",
    "Adam utilise cette perte pour corriger les poids ‚Üí le mod√®le apprend √† mieux classer les gliomes.\n",
    "\n",
    "\n",
    "__ En r√©sum√© simple :\n",
    "\n",
    "Tu donnes une image au mod√®le.\n",
    "\n",
    "Le mod√®le pr√©dit des probabilit√©s.\n",
    "\n",
    "Categorical Crossentropy mesure l‚Äôerreur de pr√©diction.\n",
    "\n",
    "Adam ajuste les poids du CNN pour r√©duire cette erreur.\n",
    "\n",
    "R√©p√®te pour toutes les images, epoch apr√®s epoch ‚Üí le mod√®le s‚Äôam√©liore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc277b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilation du mod√®le\n",
    "model.compile(\n",
    "    optimizer='adam',                 \n",
    "    loss='categorical_crossentropy',  \n",
    "    metrics=['accuracy']              \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd497260",
   "metadata": {},
   "source": [
    "4_ Utiliser les fonctions model.summary() et plot_model() pour v√©rifier l‚Äôarchitecture du mod√®le.\n",
    "**** model.summary() va afficher dans la console toutes les couches du mod√®le, leurs sorties et le nombre de param√®tres.\n",
    "\n",
    "plot_model() cr√©e un fichier image (cnn_model.png) avec une repr√©sentation graphique du CNN. show_shapes=True permet de voir la taille des sorties de chaque couche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf89570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Afficher un r√©sum√© du mod√®le\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085ea2f9",
   "metadata": {},
   "source": [
    "5_ D√©terminer les hyperparam√®tres (taux d‚Äôapprentissage, nombre d‚Äô√©poques, taille de batch).\n",
    "\n",
    "Les hyperparam√®tres d√©finissent le comportement du processus d‚Äôapprentissage d‚Äôun mod√®le de deep learning. Ils influencent directement la vitesse de convergence, la stabilit√© et la performance finale du mod√®le.\n",
    "\n",
    "1Ô∏è‚É£ Taux d‚Äôapprentissage (learning_rate)\n",
    "\n",
    "D√©finition :\n",
    "Le taux d‚Äôapprentissage contr√¥le l‚Äôamplitude des mises √† jour appliqu√©es aux poids du mod√®le apr√®s chaque it√©ration.\n",
    "Il s‚Äôagit du param√®tre le plus critique dans l‚Äôoptimisation.\n",
    "\n",
    "Valeur typique : 0.001\n",
    "(valeur par d√©faut de nombreux optimiseurs comme Adam)\n",
    "\n",
    "Effets :\n",
    "\n",
    "üîπ Trop √©lev√© ‚Üí l‚Äôalgorithme saute le minimum et ne converge pas.\n",
    "\n",
    "üîπ Trop faible ‚Üí la convergence devient tr√®s lente ou le mod√®le reste bloqu√© dans un minimum local.\n",
    "\n",
    "Recommandation :\n",
    "Tester plusieurs valeurs dans une plage comme [0.0001, 0.005], ou utiliser un scheduler pour l‚Äôajuster dynamiquement.\n",
    "\n",
    "\n",
    "2Ô∏è‚É£ Nombre d‚Äô√©poques (epochs)\n",
    "\n",
    "D√©finition :\n",
    "Une √©poque correspond √† un passage complet sur l‚Äôensemble des donn√©es d‚Äôentra√Ænement.\n",
    "Le nombre d‚Äô√©poques d√©termine combien de fois le mod√®le revoit les donn√©es pendant l‚Äôapprentissage.\n",
    "\n",
    "Valeur typique : 10 √† 50, selon la taille du dataset et la complexit√© du mod√®le.\n",
    "\n",
    "Effets :\n",
    "\n",
    "üîπ Trop faible ‚Üí sous-apprentissage (le mod√®le n‚Äôa pas encore appris les motifs).\n",
    "\n",
    "üîπ Trop √©lev√© ‚Üí surapprentissage (le mod√®le m√©morise le dataset et perd en g√©n√©ralisation).\n",
    "\n",
    "Astuce :\n",
    "Utiliser un callback comme EarlyStopping pour arr√™ter l‚Äôentra√Ænement quand la performance sur la validation cesse d‚Äôaugmenter.\n",
    "\n",
    "3Ô∏è‚É£ Taille de batch (batch_size)\n",
    "\n",
    "D√©finition :\n",
    "Le batch size indique le nombre d‚Äô√©chantillons trait√©s avant chaque mise √† jour des poids.\n",
    "Chaque lot (batch) est utilis√© pour calculer le gradient moyen avant la mise √† jour.\n",
    "\n",
    "Valeur typique : 16, 32, ou 64 selon la m√©moire GPU disponible.\n",
    "\n",
    "Effets :\n",
    "\n",
    "üîπ Batch petit ‚Üí apprentissage plus bruit√© mais meilleure g√©n√©ralisation.\n",
    "\n",
    "üîπ Batch grand ‚Üí apprentissage plus stable mais besoin de plus de m√©moire.\n",
    "\n",
    "Recommandation :\n",
    "Commencer avec 32, puis ajuster selon les ressources mat√©rielles et la stabilit√© de la loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8796267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparam√®tres principaux\n",
    "learning_rate = 0.001   # Taux d'apprentissage pour Adam\n",
    "epochs = 20             # Nombre d'√©poques pour l'entra√Ænement\n",
    "batch_size = 32         # Nombre d'images trait√©es √† chaque it√©ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c465d81b",
   "metadata": {},
   "source": [
    "6_Utiliser la biblioth√®que time pour mesurer la dur√©e de l‚Äôentra√Ænement.\n",
    "\n",
    "La biblioth√®que time permet de mesurer le temps √©coul√© entre le d√©but et la fin de l‚Äôentra√Ænement d‚Äôun mod√®le de machine learning ou deep learning.\n",
    "C‚Äôest utile pour :\n",
    "\n",
    "√âvaluer les performances du mod√®le et du mat√©riel (CPU/GPU)\n",
    "\n",
    "Comparer la vitesse d‚Äôentra√Ænement entre diff√©rents mod√®les ou configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5beb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[checkpoint]\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Dur√©e totale de l'entra√Ænement : {training_time:.2f} secondes\")\n",
    "print(f\"Dur√©e totale en minute : {training_time/60:.2f} minutes\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
